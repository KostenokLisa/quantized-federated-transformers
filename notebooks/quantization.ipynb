{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "8Ji1iLhW2eLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PRpKL7j9PrB"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWOj0qDS9Tit"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTnMHgBt-LE5"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTQFEZAR9ULB"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.12.0\n",
        "!pip install torchtext==0.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLyH5koG9XAT"
      },
      "outputs": [],
      "source": [
        "!pip install torchdata==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QX9Exbj89Y_n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import wget \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import transformers as ppb\n",
        "import random\n",
        "import warnings\n",
        "import math\n",
        "import copy\n",
        "from packaging import version\n",
        "from torch import Tensor, nn\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dlv7x0JY9bxf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "oraovivw3hkC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import Multi30k"
      ],
      "metadata": {
        "id": "W3lSQcdK3xrl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MarianConfig, PreTrainedModel\n",
        "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqModelOutput, Seq2SeqLMOutput"
      ],
      "metadata": {
        "id": "aysbyIM64C_T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668d445c-6a5a-43fb-c04b-cec1712ace43",
        "id": "bI_mRLez4RNO"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/HBFPEmulator/getting_started\")"
      ],
      "metadata": {
        "id": "dBvrZ5vi4RNP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bfp_ops import BFPLinear, BFPConv2d, unpack_bfp_args"
      ],
      "metadata": {
        "id": "04MMLDsi4RNP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional, Tuple, Union"
      ],
      "metadata": {
        "id": "WnGdS_Wp4izs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.marian.modeling_marian import _expand_mask, _make_causal_mask, shift_tokens_right"
      ],
      "metadata": {
        "id": "5G-bJO4t8-eP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.marian.modeling_marian import MarianSinusoidalPositionalEmbedding"
      ],
      "metadata": {
        "id": "o4_9YA1r9JKh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.activations import get_activation, ACT2FN, SiLUActivation"
      ],
      "metadata": {
        "id": "iZE_9UrEsr-M"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation"
      ],
      "metadata": {
        "id": "ElJ57Kbz8Dlj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZydXEjbODM2"
      },
      "source": [
        "## Baseline for MT + utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ-R5o4f-CDP",
        "outputId": "64114f5d-6950-4675-b44a-93d46271562a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 29001\n",
            "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Two young, White males are outside near many bushes.')\n"
          ]
        }
      ],
      "source": [
        "train_iter = Multi30k(split=\"train\")\n",
        "\n",
        "# torchtext.datasets.DatasetName yield exhaustible IterableDataset.\n",
        "# To fix this we convert our dataset to a list.\n",
        "train_data = list(train_iter)\n",
        " \n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSqxcjAI-F1w",
        "outputId": "65b0f131-d036-4b08-8c5d-3528a0f13401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test examples: 1000\n",
            "('Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.', 'A man in an orange hat starring at something.')\n"
          ]
        }
      ],
      "source": [
        "test_iter = Multi30k(split=\"test\")\n",
        "test_data = list(test_iter)\n",
        "print(f\"Number of test examples: {len(test_data)}\")\n",
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpWSsmZk-QTZ"
      },
      "outputs": [],
      "source": [
        "model_name = 'Helsinki-NLP/opus-mt-de-en'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QaGi59LsAWf2"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available!\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "U_UtgK98AZWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dabe080-4c53-4edd-87a6-11696908990b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianMTModel(\n",
              "  (model): MarianModel(\n",
              "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
              "    (encoder): MarianEncoder(\n",
              "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): MarianEncoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): MarianDecoder(\n",
              "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): MarianDecoderLayer(\n",
              "          (self_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MarianAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=58101, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "L8meVwYb8ZRb"
      },
      "outputs": [],
      "source": [
        "src_train_raw = []\n",
        "dst_train_raw = []\n",
        "for line in train_data:\n",
        "    src_train_raw.append(line[1].lower())\n",
        "    dst_train_raw.append(line[0].lower())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "njcLXNtn8ZRc"
      },
      "outputs": [],
      "source": [
        "src_val_raw = []\n",
        "dst_val_raw = []\n",
        "for line in test_data:\n",
        "    src_val_raw.append(line[1].lower())\n",
        "    dst_val_raw.append(line[0].lower())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ea0788-7e8d-4745-b9c4-c91a8fd170b4",
        "id": "BcIEs7868ZRc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['two young, white males are outside near many bushes.', 'several men in hard hats are operating a giant pulley system.', 'a little girl climbing into a wooden playhouse.', 'a man in a blue shirt is standing on a ladder cleaning a window.', 'two men are at the stove preparing food.', 'a man in green holds a guitar while the other man observes his shirt.', 'a man is smiling at a stuffed lion', 'a trendy girl talking on her cellphone while gliding slowly down the street.', 'a woman with a large purse is walking by a gate.', 'boys dancing on poles in the middle of the night.']\n",
            "['zwei junge weiße männer sind im freien in der nähe vieler büsche.', 'mehrere männer mit schutzhelmen bedienen ein antriebsradsystem.', 'ein kleines mädchen klettert in ein spielhaus aus holz.', 'ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster.', 'zwei männer stehen am herd und bereiten essen zu.', 'ein mann in grün hält eine gitarre, während der andere mann sein hemd ansieht.', 'ein mann lächelt einen ausgestopften löwen an.', 'ein schickes mädchen spricht mit dem handy während sie langsam die straße entlangschwebt.', 'eine frau mit einer großen geldbörse geht an einem tor vorbei.', 'jungen tanzen mitten in der nacht auf pfosten.']\n"
          ]
        }
      ],
      "source": [
        "print(src_train_raw[:10])\n",
        "print(dst_train_raw[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uObWkYuS8ZRd"
      },
      "outputs": [],
      "source": [
        "src_train = tokenizer(src_train_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "dest_train = tokenizer(dst_train_raw, padding=True, truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "85yh1Qrm8ZRd"
      },
      "outputs": [],
      "source": [
        "src_val = tokenizer(src_val_raw, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "dest_val = tokenizer(dst_val_raw, padding=True, truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "y8rOiSUH8ZRd"
      },
      "outputs": [],
      "source": [
        "def estimate_bleu(model, inp_idx, out_idx, batch_size):\n",
        "    tokenizer_W = WordPunctTokenizer()\n",
        "    with torch.no_grad():\n",
        "        translations = []\n",
        "        inp_idx = inp_idx.to(device)\n",
        "        prev_i = 0\n",
        "        for i in tqdm.notebook.tqdm(range(batch_size, len(inp_idx) + batch_size, batch_size)):\n",
        "            output = model.generate(inp_idx[prev_i:i]).cpu()\n",
        "            prev_i = i\n",
        "            translations.extend(tokenizer.batch_decode(output, skip_special_tokens=True))\n",
        "        actual = tokenizer.batch_decode(out_idx, skip_special_tokens=True)\n",
        "        return corpus_bleu(\n",
        "            [[tokenizer_W.tokenize(ref)] for ref in actual],\n",
        "            [tokenizer_W.tokenize(trans.lower()) for trans in translations],\n",
        "            smoothing_function=lambda precisions, **kw: [p + 1.0 / p.denominator for p in precisions]\n",
        "            ) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantized model"
      ],
      "metadata": {
        "id": "iFZTnI1g8e7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = MarianConfig(None)\n",
        "print(config.d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5tI71V-_3ik",
        "outputId": "9cb12e62-6115-4425-de6e-921ad556fed9"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "        bfp_args={}\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        if (self.head_dim * num_heads) != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "                f\" and `num_heads`: {num_heads}).\"\n",
        "            )\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = BFPLinear(embed_dim, embed_dim, bias=bias, **bfp_args)\n",
        "        self.v_proj = BFPLinear(embed_dim, embed_dim, bias=bias, **bfp_args)\n",
        "        self.q_proj = BFPLinear(embed_dim, embed_dim, bias=bias, **bfp_args)\n",
        "        self.out_proj = BFPLinear(embed_dim, embed_dim, bias=bias, **bfp_args)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "\n",
        "        bsz, tgt_len, _ = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.view(*proj_shape)\n",
        "        value_states = value_states.view(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
        "                raise ValueError(\n",
        "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            if layer_head_mask.size() != (self.num_heads,):\n",
        "                raise ValueError(\n",
        "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
        "                    f\" {layer_head_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "        ## quant\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "        ## dequant\n",
        "\n",
        "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "        return attn_output, attn_weights_reshaped, past_key_value\n"
      ],
      "metadata": {
        "id": "MLia4Wat22rl"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: MarianConfig, bfp_args={}):\n",
        "        super().__init__()\n",
        "        # self.embed_dim = config.d_model\n",
        "        self.embed_dim = 512\n",
        "        self.self_attn = BFPMarianAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=8,\n",
        "            dropout=0.0,\n",
        "            # **bfp_args,\n",
        "            bfp_args=bfp_args,\n",
        "        )\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout = 0.1\n",
        "\n",
        "        self.activation_fn = ACT2FN[\"silu\"]\n",
        "        self.activation_dropout = 0.0\n",
        "        self.fc1 = BFPLinear(self.embed_dim, 2048, **bfp_args)\n",
        "        self.fc2 = BFPLinear(2048, self.embed_dim, **bfp_args)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.FloatTensor,\n",
        "        attention_mask: torch.FloatTensor,\n",
        "        layer_head_mask: torch.FloatTensor,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
        "      \n",
        "        residual = hidden_states\n",
        "        hidden_states, attn_weights, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        if hidden_states.dtype == torch.float16 and (\n",
        "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
        "        ):\n",
        "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
        "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "HE95byLX0x4v"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianPreTrainedModel(PreTrainedModel):\n",
        "    # config_class = MarianConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "\n",
        "    def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalPositionalEmbedding]):\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, MarianSinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, (BFPMarianDecoder, BFPMarianEncoder)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = 58100\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"decoder_input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs"
      ],
      "metadata": {
        "id": "wPq3y327czp-"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianEncoder(BFPMarianPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    [`MarianEncoderLayer`].\n",
        "    Args:\n",
        "        config: MarianConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] = None, bfp_args={}):\n",
        "        super().__init__(config, bfp_args)\n",
        "\n",
        "        self.dropout = 0.1\n",
        "        self.layerdrop = 0.0\n",
        "\n",
        "        embed_dim = 512\n",
        "        self.padding_idx = 58100\n",
        "        self.max_source_positions = 512\n",
        "        self.embed_scale = 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(58101, 512, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = MarianSinusoidalPositionalEmbedding(\n",
        "            512, embed_dim, self.padding_idx\n",
        "        )\n",
        "        self.layers = nn.ModuleList([BFPMarianEncoderLayer(config, bfp_args) for _ in range(6)])\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutput]:\n",
        "   \n",
        "        output_attentions = output_attentions if output_attentions is not None else False \n",
        "        output_hidden_states = output_hidden_states if output_hidden_states is not None else False\n",
        "        return_dict = return_dict if return_dict is not None else False\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )"
      ],
      "metadata": {
        "id": "fjtwQk3VO0xx"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianDecoderLayer(nn.Module):\n",
        "    def __init__(self, config: MarianConfig, bfp_args={}):\n",
        "        super().__init__()\n",
        "        self.embed_dim = 512\n",
        "\n",
        "        self.self_attn = BFPMarianAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=8,\n",
        "            dropout=0.0,\n",
        "            is_decoder=True,\n",
        "            bfp_args=bfp_args\n",
        "        )\n",
        "        self.dropout = 0.1\n",
        "        self.activation_fn = ACT2FN[\"silu\"]\n",
        "        self.activation_dropout = 0.0\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = BFPMarianAttention(\n",
        "            self.embed_dim,\n",
        "            8,\n",
        "            dropout=0.0,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.fc1 = BFPLinear(512, 2048, **bfp_args)\n",
        "        self.fc2 = BFPLinear(2048, 512, **bfp_args)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = True,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "      \n",
        "        residual = hidden_states\n",
        "\n",
        "        # Self Attention\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        cross_attn_present_key_value = None\n",
        "        cross_attn_weights = None\n",
        "        if encoder_hidden_states is not None:\n",
        "            residual = hidden_states\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
        "                hidden_states=hidden_states,\n",
        "                key_value_states=encoder_hidden_states,\n",
        "                attention_mask=encoder_attention_mask,\n",
        "                layer_head_mask=cross_attn_layer_head_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "            hidden_states = residual + hidden_states\n",
        "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
        "\n",
        "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights, cross_attn_weights)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "dIgrNG8K7XN3"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianDecoder(BFPMarianPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`MarianDecoderLayer`]\n",
        "    Args:\n",
        "        config: MarianConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] = None, bfp_args={}):\n",
        "        super().__init__(config, bfp_args)\n",
        "        self.dropout = 0.1\n",
        "        self.layerdrop = 0.0\n",
        "        self.padding_idx = 58100\n",
        "        self.max_target_positions = 512\n",
        "        self.embed_scale = 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(58101, 512, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = MarianSinusoidalPositionalEmbedding(\n",
        "            512, 512, self.padding_idx\n",
        "        )\n",
        "        self.layers = nn.ModuleList([BFPMarianDecoderLayer(config, bfp_args) for _ in range(6)])\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
        "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "        # create causal mask\n",
        "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "        combined_attention_mask = None\n",
        "        if input_shape[-1] > 1:\n",
        "            combined_attention_mask = _make_causal_mask(\n",
        "                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n",
        "            ).to(inputs_embeds.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "            combined_attention_mask = (\n",
        "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "            )\n",
        "\n",
        "        return combined_attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "       \n",
        "        output_attentions = output_attentions if output_attentions is not None else False\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else False\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else False\n",
        "        return_dict = return_dict if return_dict is not None else False\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        attention_mask = self._prepare_decoder_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "\n",
        "        # expand encoder attention mask\n",
        "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
        "\n",
        "        hidden_states = inputs_embeds + positions\n",
        "\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
        "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
        "            if attn_mask is not None:\n",
        "                assert attn_mask.size()[0] == (len(self.layers)), (\n",
        "                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
        "                    f\" {head_mask.size()[0]}.\"\n",
        "                )\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    # logger.warning(\n",
        "                        # \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                    # )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        # None for past_key_value\n",
        "                        return module(*inputs, output_attentions, use_cache)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(decoder_layer),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    head_mask[idx] if head_mask is not None else None,\n",
        "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
        "                    None,\n",
        "                )\n",
        "            else:\n",
        "\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    encoder_attention_mask=encoder_attention_mask,\n",
        "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                    cross_attn_layer_head_mask=(\n",
        "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
        "                    ),\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "                if encoder_hidden_states is not None:\n",
        "                    all_cross_attentions += (layer_outputs[2],)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "-JiyfjCVoIEa"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianModel(BFPMarianPreTrainedModel):\n",
        "    def __init__(self, config: MarianConfig, bfp_args={}):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = 58100, 58101\n",
        "\n",
        "        # We always use self.shared for token embeddings to ensure compatibility with all marian models\n",
        "        self.shared = nn.Embedding(vocab_size, 512, padding_idx)\n",
        "        encoder_embed_tokens = decoder_embed_tokens = self.shared\n",
        "        encoder_embed_tokens = copy.deepcopy(self.shared)\n",
        "        decoder_embed_tokens = copy.deepcopy(self.shared)\n",
        "        # self.shared = None\n",
        "\n",
        "        self.encoder = BFPMarianEncoder(config, encoder_embed_tokens, bfp_args)\n",
        "        self.decoder = BFPMarianDecoder(config, decoder_embed_tokens, bfp_args)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        # This will return shared embeddings if they are shared else specific to encoder.\n",
        "        return self.get_encoder().get_input_embeddings()\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        if self.config.share_encoder_decoder_embeddings:\n",
        "            self.shared = value\n",
        "            self.encoder.embed_tokens = self.shared\n",
        "            self.decoder.embed_tokens = self.shared\n",
        "        else:  # if not shared only set encoder embeedings\n",
        "            self.encoder.embed_tokens = value\n",
        "\n",
        "    def get_decoder_input_embeddings(self):\n",
        "        if self.config.share_encoder_decoder_embeddings:\n",
        "            raise ValueError(\n",
        "                \"`get_decoder_input_embeddings` should not be called if `config.share_encoder_decoder_embeddings` \"\n",
        "                \"is `True`. Please use `get_input_embeddings` instead.\"\n",
        "            )\n",
        "        return self.get_decoder().get_input_embeddings()\n",
        "\n",
        "    def set_decoder_input_embeddings(self, value):\n",
        "        if self.config.share_encoder_decoder_embeddings:\n",
        "            raise ValueError(\n",
        "                \"`config.share_encoder_decoder_embeddings` is set to `True` meaning the decoder input embeddings \"\n",
        "                \"are shared with the encoder. In order to set the decoder input embeddings, you should simply set \"\n",
        "                \"the encoder input embeddings by calling `set_input_embeddings` with the appropriate embeddings.\"\n",
        "            )\n",
        "        self.decoder.embed_tokens = value\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def resize_decoder_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        if self.config.share_encoder_decoder_embeddings:\n",
        "            raise ValueError(\n",
        "                \"`resize_decoder_token_embeddings` should not be called if `config.share_encoder_decoder_embeddings` \"\n",
        "                \"is `True`. Please use `resize_token_embeddings` instead.\"\n",
        "            )\n",
        "\n",
        "        old_embeddings = self.get_decoder_input_embeddings()\n",
        "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
        "        self.set_decoder_input_embeddings(new_embeddings)\n",
        "\n",
        "        model_embeds = self.get_decoder_input_embeddings()\n",
        "\n",
        "        if new_num_tokens is None:\n",
        "            return model_embeds\n",
        "\n",
        "        # Update base model and current model config\n",
        "        self.config.decoder_vocab_size = new_num_tokens\n",
        "\n",
        "        # Tie weights again if needed\n",
        "        self.tie_weights()\n",
        "\n",
        "        return model_embeds\n",
        "\n",
        "    # @add_start_docstrings_to_model_forward(MARIAN_INPUTS_DOCSTRING)\n",
        "    # @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Union[Tuple[torch.Tensor], BaseModelOutput]] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Seq2SeqModelOutput:\n",
        "        \n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "      # @add_start_docstrings(\n",
        "   # \"The Marian Model with a language modeling head. Can be used for summarization.\", MARIAN_START_DOCSTRING\n",
        "# )"
      ],
      "metadata": {
        "id": "NNadqpYZapSt"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BFPMarianMTModel(BFPMarianPreTrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [\n",
        "        r\"final_logits_bias\",\n",
        "        r\"encoder.version\",\n",
        "        r\"decoder.version\",\n",
        "        r\"lm_head.weight\",\n",
        "        r\"embed_positions\",\n",
        "    ]\n",
        "\n",
        "    _keys_to_ignore_on_save = [\"model.encoder.embed_positions.weight\", \"model.decoder.embed_positions.weight\"]\n",
        "\n",
        "    def __init__(self, config: MarianConfig, bfp_args={}):\n",
        "        super().__init__(config)\n",
        "        self.model = BFPMarianModel(config, bfp_args)\n",
        "\n",
        "        # target_vocab_size = config.vocab_size if config.share_encoder_decoder_embeddings else config.decoder_vocab_size\n",
        "        target_vocab_size = 58101\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, target_vocab_size)))\n",
        "        self.lm_head = BFPLinear(512, target_vocab_size, bias=False, bfp_args = bfp_args)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        if self.config.share_encoder_decoder_embeddings:\n",
        "            self._resize_final_logits_bias(new_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        old_embeddings = self.get_input_embeddings()\n",
        "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
        "        self.set_input_embeddings(new_embeddings)\n",
        "\n",
        "        # update config.decoder_vocab_size if embeddings are tied\n",
        "        if self.config.share_encoder_decoder_embeddings:\n",
        "            self.config.decoder_vocab_size = new_num_tokens\n",
        "\n",
        "        # if word embeddings are not tied, make sure that lm head is resized as well\n",
        "        if (\n",
        "            self.config.share_encoder_decoder_embeddings\n",
        "            and self.get_output_embeddings() is not None\n",
        "            and not self.config.tie_word_embeddings\n",
        "        ):\n",
        "            old_lm_head = self.get_output_embeddings()\n",
        "            new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens)\n",
        "            self.set_output_embeddings(new_lm_head)\n",
        "\n",
        "        return self.get_input_embeddings()\n",
        "\n",
        "    def resize_decoder_token_embeddings(self, new_num_tokens):\n",
        "        if self.config.share_encoder_decoder_embeddings:\n",
        "            raise ValueError(\n",
        "                \"`resize_decoder_token_embeddings` should not be called if `config.share_encoder_decoder_embeddings` \"\n",
        "                \"is `True`. Please use `resize_token_embeddings` instead.\"\n",
        "            )\n",
        "\n",
        "        old_embeddings = self.model.get_decoder_input_embeddings()\n",
        "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
        "        self.model.set_decoder_input_embeddings(new_embeddings)\n",
        "\n",
        "        # if word embeddings are not tied, make sure that lm head is resized as well\n",
        "        if self.get_output_embeddings() is not None and not self.config.tie_word_embeddings:\n",
        "            old_lm_head = self.get_output_embeddings()\n",
        "            new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens)\n",
        "            self.set_output_embeddings(new_lm_head)\n",
        "\n",
        "        model_embeds = self.model.get_decoder_input_embeddings()\n",
        "\n",
        "        if new_num_tokens is None:\n",
        "            return model_embeds\n",
        "\n",
        "        # Update base model and current model config\n",
        "        self.config.decoder_vocab_size = new_num_tokens\n",
        "\n",
        "        # Tie weights again if needed\n",
        "        self.tie_weights()\n",
        "\n",
        "        self._resize_final_logits_bias(new_num_tokens)\n",
        "\n",
        "        return model_embeds\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
        "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings: nn.Embedding):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\"\n",
        "        Tie the weights between the input embeddings and the output embeddings.\n",
        "        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n",
        "        weights instead.\n",
        "        \"\"\"\n",
        "        output_embeddings = self.get_output_embeddings()\n",
        "        if output_embeddings is not None and getattr(self.config, \"tie_word_embeddings\", True):\n",
        "            # if embeddings are shared this will return shared embeddings otherwise decoder embed_tokens\n",
        "            word_embeddings = self.get_decoder().get_input_embeddings()\n",
        "            self._tie_or_clone_weights(output_embeddings, word_embeddings)\n",
        "\n",
        "        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n",
        "            if hasattr(self, self.base_model_prefix):\n",
        "                self = getattr(self, self.base_model_prefix)\n",
        "            self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, \"_tie_weights\"):\n",
        "                module._tie_weights()\n",
        "\n",
        "    # @add_start_docstrings_to_model_forward(MARIAN_INPUTS_DOCSTRING)\n",
        "    # @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    # @add_end_docstrings(MARIAN_GENERATION_EXAMPLE)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Union[Tuple[torch.Tensor], BaseModelOutput]] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Seq2SeqLMOutput:\n",
        "       \n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "            if decoder_input_ids is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.decoder_vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids: torch.LongTensor,\n",
        "        past: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        encoder_outputs: Optional[Union[Tuple[torch.Tensor], BaseModelOutput]] = None,\n",
        "        **kwargs,\n",
        "    ) -> Dict:\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"decoder_head_mask\": decoder_head_mask,\n",
        "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    def adjust_logits_during_generation(self, logits, cur_len):\n",
        "        logits[:, self.config.pad_token_id] = float(\"-inf\")  # never predict pad token.\n",
        "        return logits\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past\n"
      ],
      "metadata": {
        "id": "VaqZqURA972w"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bfp quantization arguments\n",
        "args = {\"num_format\": \"bfp\", \"rounding_mode\": \"stoc\", \"mant_bits\": 8, \n",
        "        \"bfp_tile_size\": 0, \"weight_mant_bits\": 16, \"device\": \"cpu\"}\n",
        "model = BFPMarianMTModel(config, args)"
      ],
      "metadata": {
        "id": "z1dVNe7zB0Ma"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantized model vs basic model"
      ],
      "metadata": {
        "id": "WgBYE594Erg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pretrained MarianMT model from transformers and estimate its score"
      ],
      "metadata": {
        "id": "H_yMPVrKzRNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Helsinki-NLP/opus-mt-de-en'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "native_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "a2Lxec8eDCfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = dest_train[\"input_ids\"]\n",
        "train_labels = src_train[\"input_ids\"]\n",
        "train_mask = dest_train[\"attention_mask\"] \n",
        "val_ids = dest_val[\"input_ids\"]\n",
        "val_labels = src_val[\"input_ids\"]\n",
        "val_mask = dest_val[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "MqQdA7W1z7ro"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_bleu(native_model, val_ids[:32], val_labels[:32], 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "b1f9e0c2894841a78b645ee1c55ebdc9",
            "ba87ea3138e04f16919eaced6fc7ef9f",
            "881ef990183c49bd9381e9f4bfd63fe2",
            "5790cbe2a2514bd8817934daff05fc5d",
            "8b990ce0290d4fc5b1a3ecc8445db475",
            "b7dc9c88a641465ba3c0111992634896",
            "c4389a3ee2824d44acf20cb5f48fba79",
            "b82e37cfb0f54b2587821e11374d61f2",
            "f416fd8e639b4bbd9c7dc95f7c18c4bc",
            "d26feafca68d4166ac650e49ad815ab2",
            "371a012601244796a02bce7c69b4daee"
          ]
        },
        "id": "i5fOOfyLD0XC",
        "outputId": "671e9821-8256-49fb-e2a5-f18af7edc535"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1f9e0c2894841a78b645ee1c55ebdc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41.17538054581144"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(native_model.state_dict(), \"init.pth\")"
      ],
      "metadata": {
        "id": "42lj7zaO0bF6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"init.pth\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyDmE9j0JG5E",
        "outputId": "139ded1b-155b-4d71-dba3-77929de9cf59"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BFPMarianMTModel(\n",
              "  (model): BFPMarianModel(\n",
              "    (shared): Embedding(58101, 512, padding_idx=58100)\n",
              "    (encoder): BFPMarianEncoder(\n",
              "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0): BFPMarianEncoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BFPMarianEncoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BFPMarianEncoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BFPMarianEncoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BFPMarianEncoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BFPMarianEncoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): BFPMarianDecoder(\n",
              "      (embed_tokens): Embedding(58101, 512, padding_idx=58100)\n",
              "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
              "      (layers): ModuleList(\n",
              "        (0): BFPMarianDecoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BFPMarianDecoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BFPMarianDecoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BFPMarianDecoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BFPMarianDecoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BFPMarianDecoderLayer(\n",
              "          (self_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (activation_fn): SiLUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BFPMarianAttention(\n",
              "            (k_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): BFPLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): BFPLinear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): BFPLinear(in_features=2048, out_features=512, bias=True)\n",
              "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): BFPLinear(in_features=512, out_features=58101, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "dufp9cA5AsFQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "2fa4064f593c40449de26a9b6d5b3055",
            "a8f8b0e2f2d04f4e855a8bcacdb26c65",
            "2b0e8ebb53e249fe9c2b9ec49ee6bb7b",
            "0ac4f46f64124ac89a249b4ad7f1a58e",
            "e621fe4349a7418ba8b0ffda45c77947",
            "3c6e8a96dd8c46e0b28b1a356820b75a",
            "d2ef88315bb040759fdc81da9ce6397d",
            "f4a9d69c6b2543ab8287859e167f101c",
            "aa01d44d8a5d47cfbcaf56698d4d4f43",
            "468bb43204084f099a6530b0b55bbeb4",
            "cbd3e22f409849cb990ec0c1f1edbc21"
          ]
        },
        "outputId": "fec74890-efa2-4213-dcbc-e6d9cd502273"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fa4064f593c40449de26a9b6d5b3055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35.460835905298524"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "train_ids = dest_train[\"input_ids\"]\n",
        "train_labels = src_train[\"input_ids\"]\n",
        "train_mask = dest_train[\"attention_mask\"] \n",
        "val_ids = dest_val[\"input_ids\"]\n",
        "val_labels = src_val[\"input_ids\"]\n",
        "val_mask = dest_val[\"attention_mask\"]\n",
        "\n",
        "estimate_bleu(model, val_ids[:32], val_labels[:32], 16) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "native_model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3QWajwS7w5O",
        "outputId": "46594fb2-8346-4d9d-8b21-33bf461167fa"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MarianConfig {\n",
              "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-de-en\",\n",
              "  \"_num_labels\": 3,\n",
              "  \"activation_dropout\": 0.0,\n",
              "  \"activation_function\": \"swish\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"MarianMTModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bad_words_ids\": [\n",
              "    [\n",
              "      58100\n",
              "    ]\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.0,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 512,\n",
              "  \"decoder_attention_heads\": 8,\n",
              "  \"decoder_ffn_dim\": 2048,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 6,\n",
              "  \"decoder_start_token_id\": 58100,\n",
              "  \"decoder_vocab_size\": 58101,\n",
              "  \"dropout\": 0.1,\n",
              "  \"encoder_attention_heads\": 8,\n",
              "  \"encoder_ffn_dim\": 2048,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 0,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_length\": 512,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"marian\",\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": false,\n",
              "  \"num_beams\": 4,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 58100,\n",
              "  \"scale_embedding\": true,\n",
              "  \"share_encoder_decoder_embeddings\": true,\n",
              "  \"static_position_embeddings\": true,\n",
              "  \"transformers_version\": \"4.21.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 58101\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "quantization.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1f9e0c2894841a78b645ee1c55ebdc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba87ea3138e04f16919eaced6fc7ef9f",
              "IPY_MODEL_881ef990183c49bd9381e9f4bfd63fe2",
              "IPY_MODEL_5790cbe2a2514bd8817934daff05fc5d"
            ],
            "layout": "IPY_MODEL_8b990ce0290d4fc5b1a3ecc8445db475"
          }
        },
        "ba87ea3138e04f16919eaced6fc7ef9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7dc9c88a641465ba3c0111992634896",
            "placeholder": "​",
            "style": "IPY_MODEL_c4389a3ee2824d44acf20cb5f48fba79",
            "value": "100%"
          }
        },
        "881ef990183c49bd9381e9f4bfd63fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b82e37cfb0f54b2587821e11374d61f2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f416fd8e639b4bbd9c7dc95f7c18c4bc",
            "value": 2
          }
        },
        "5790cbe2a2514bd8817934daff05fc5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d26feafca68d4166ac650e49ad815ab2",
            "placeholder": "​",
            "style": "IPY_MODEL_371a012601244796a02bce7c69b4daee",
            "value": " 2/2 [00:16&lt;00:00,  8.46s/it]"
          }
        },
        "8b990ce0290d4fc5b1a3ecc8445db475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7dc9c88a641465ba3c0111992634896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4389a3ee2824d44acf20cb5f48fba79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b82e37cfb0f54b2587821e11374d61f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f416fd8e639b4bbd9c7dc95f7c18c4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d26feafca68d4166ac650e49ad815ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "371a012601244796a02bce7c69b4daee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fa4064f593c40449de26a9b6d5b3055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8f8b0e2f2d04f4e855a8bcacdb26c65",
              "IPY_MODEL_2b0e8ebb53e249fe9c2b9ec49ee6bb7b",
              "IPY_MODEL_0ac4f46f64124ac89a249b4ad7f1a58e"
            ],
            "layout": "IPY_MODEL_e621fe4349a7418ba8b0ffda45c77947"
          }
        },
        "a8f8b0e2f2d04f4e855a8bcacdb26c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c6e8a96dd8c46e0b28b1a356820b75a",
            "placeholder": "​",
            "style": "IPY_MODEL_d2ef88315bb040759fdc81da9ce6397d",
            "value": "100%"
          }
        },
        "2b0e8ebb53e249fe9c2b9ec49ee6bb7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4a9d69c6b2543ab8287859e167f101c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa01d44d8a5d47cfbcaf56698d4d4f43",
            "value": 2
          }
        },
        "0ac4f46f64124ac89a249b4ad7f1a58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_468bb43204084f099a6530b0b55bbeb4",
            "placeholder": "​",
            "style": "IPY_MODEL_cbd3e22f409849cb990ec0c1f1edbc21",
            "value": " 2/2 [00:08&lt;00:00,  4.22s/it]"
          }
        },
        "e621fe4349a7418ba8b0ffda45c77947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c6e8a96dd8c46e0b28b1a356820b75a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ef88315bb040759fdc81da9ce6397d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4a9d69c6b2543ab8287859e167f101c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa01d44d8a5d47cfbcaf56698d4d4f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "468bb43204084f099a6530b0b55bbeb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd3e22f409849cb990ec0c1f1edbc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}